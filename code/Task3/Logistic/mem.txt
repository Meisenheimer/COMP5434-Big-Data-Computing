Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.9 MiB    735.9 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.9 MiB      0.0 MiB           1       print("Loading data.")
   106    787.0 MiB     51.0 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    804.5 MiB     17.5 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    804.5 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    804.5 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    804.5 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    804.5 MiB      0.0 MiB           1       print("Training and testing.")
   117    804.5 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    804.5 MiB      0.0 MiB           1       model = getModel(args)
   120    851.1 MiB     46.6 MiB           1       model.fit(train_x, train_y)
   121    831.9 MiB    -19.2 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    834.4 MiB      2.5 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    834.4 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    834.4 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    854.4 MiB     20.0 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    854.4 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    854.4 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    854.6 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    854.6 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    854.6 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    741.3 MiB    741.3 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    741.3 MiB      0.0 MiB           1       print("Loading data.")
   106    791.7 MiB     50.4 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    824.7 MiB     33.0 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    824.7 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    871.2 MiB     46.5 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    891.1 MiB     19.9 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    891.1 MiB      0.0 MiB           1       print("Training and testing.")
   117    891.1 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    891.1 MiB      0.0 MiB           1       model = getModel(args)
   120    937.7 MiB     46.7 MiB           1       model.fit(train_x, train_y)
   121    895.2 MiB    -42.5 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    895.3 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    895.3 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    895.3 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    915.2 MiB     19.9 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    915.2 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    915.2 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    915.4 MiB     -1.1 MiB       89934           for i in range(len(test_id)):
   133    915.4 MiB     -0.9 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    915.4 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    736.2 MiB    736.2 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    736.2 MiB      0.0 MiB           1       print("Loading data.")
   106    786.7 MiB     50.5 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    805.7 MiB     19.0 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    805.7 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    898.7 MiB     93.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    938.5 MiB     39.8 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    938.5 MiB      0.0 MiB           1       print("Training and testing.")
   117    938.5 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    938.5 MiB      0.0 MiB           1       model = getModel(args)
   120   1008.2 MiB     69.8 MiB           1       model.fit(train_x, train_y)
   121    942.5 MiB    -65.8 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    944.5 MiB      2.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    944.5 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    944.5 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    974.4 MiB     29.9 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    974.4 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    974.4 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    974.5 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    974.5 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    974.5 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.4 MiB    735.4 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.4 MiB      0.0 MiB           1       print("Loading data.")
   106    786.2 MiB     50.8 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    817.9 MiB     31.7 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    817.9 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    817.9 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    817.9 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    817.9 MiB      0.0 MiB           1       print("Training and testing.")
   117    817.9 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    817.9 MiB      0.0 MiB           1       model = getModel(args)
   120    864.5 MiB     46.6 MiB           1       model.fit(train_x, train_y)
   121    845.2 MiB    -19.2 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    845.3 MiB      0.1 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    845.3 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    845.3 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    865.3 MiB     20.0 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    865.3 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    865.3 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    865.5 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    865.5 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    865.5 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    742.7 MiB    742.7 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    742.7 MiB      0.0 MiB           1       print("Loading data.")
   106    792.7 MiB     49.9 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    810.5 MiB     17.9 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    810.5 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    857.1 MiB     46.5 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    877.0 MiB     19.9 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    877.0 MiB      0.0 MiB           1       print("Training and testing.")
   117    877.0 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    877.0 MiB      0.0 MiB           1       model = getModel(args)
   120    923.5 MiB     46.5 MiB           1       model.fit(train_x, train_y)
   121    881.0 MiB    -42.5 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    882.8 MiB      1.8 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    882.8 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    882.8 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    902.7 MiB     19.9 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    902.7 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    902.7 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    903.2 MiB     -1.0 MiB       89934           for i in range(len(test_id)):
   133    903.2 MiB     -0.7 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    903.2 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    736.3 MiB    736.3 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    736.3 MiB      0.0 MiB           1       print("Loading data.")
   106    786.8 MiB     50.5 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    804.8 MiB     17.9 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    804.8 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    897.8 MiB     93.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    937.6 MiB     39.8 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    937.6 MiB      0.0 MiB           1       print("Training and testing.")
   117    937.6 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    937.6 MiB      0.0 MiB           1       model = getModel(args)
   120   1007.3 MiB     69.8 MiB           1       model.fit(train_x, train_y)
   121    941.6 MiB    -65.8 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    944.2 MiB      2.7 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    944.2 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    944.2 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    974.1 MiB     29.9 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    974.1 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    974.1 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    974.4 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    974.4 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    974.4 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.5 MiB    735.5 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.5 MiB      0.0 MiB           1       print("Loading data.")
   106    785.2 MiB     49.7 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    816.3 MiB     31.0 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    816.3 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    816.3 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    816.3 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    816.3 MiB      0.0 MiB           1       print("Training and testing.")
   117    816.3 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    816.3 MiB      0.0 MiB           1       model = getModel(args)
   120    862.9 MiB     46.6 MiB           1       model.fit(train_x, train_y)
   121    843.6 MiB    -19.2 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    843.6 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    843.6 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    843.6 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    863.6 MiB     20.0 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    863.6 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    863.6 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    863.8 MiB     -0.7 MiB       89934           for i in range(len(test_id)):
   133    863.8 MiB     -0.4 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    863.8 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    741.0 MiB    741.0 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    741.0 MiB      0.0 MiB           1       print("Loading data.")
   106    791.2 MiB     50.2 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    808.9 MiB     17.7 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    808.9 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    855.5 MiB     46.6 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    875.5 MiB     20.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    875.5 MiB      0.0 MiB           1       print("Training and testing.")
   117    875.5 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    875.5 MiB      0.0 MiB           1       model = getModel(args)
   120    922.0 MiB     46.5 MiB           1       model.fit(train_x, train_y)
   121    879.5 MiB    -42.5 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    881.8 MiB      2.3 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    881.8 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    881.8 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    901.8 MiB     20.0 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    901.8 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    901.8 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    902.1 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    902.0 MiB      0.3 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    902.1 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    734.8 MiB    734.8 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    734.8 MiB      0.0 MiB           1       print("Loading data.")
   106    785.4 MiB     50.6 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    804.1 MiB     18.7 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    804.1 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    897.4 MiB     93.3 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    937.2 MiB     39.8 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    937.2 MiB      0.0 MiB           1       print("Training and testing.")
   117    937.2 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    937.2 MiB      0.0 MiB           1       model = getModel(args)
   120   1007.0 MiB     69.8 MiB           1       model.fit(train_x, train_y)
   121    941.2 MiB    -65.8 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    942.5 MiB      1.3 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    942.5 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    942.5 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    972.4 MiB     29.9 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    972.4 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    972.4 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    972.6 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    972.6 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    972.6 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.7 MiB    735.7 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.7 MiB      0.0 MiB           1       print("Loading data.")
   106    772.7 MiB     37.1 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    786.5 MiB     13.8 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    786.5 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    786.5 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    786.5 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    786.5 MiB      0.0 MiB           1       print("Training and testing.")
   117    786.5 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    786.5 MiB      0.0 MiB           1       model = getModel(args)
   120    820.4 MiB     33.8 MiB           1       model.fit(train_x, train_y)
   121    807.5 MiB    -12.9 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    810.6 MiB      3.1 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    810.6 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    810.6 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    825.1 MiB     14.5 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    825.1 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    825.1 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    825.3 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    825.3 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    825.3 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    734.5 MiB    734.5 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    734.5 MiB      0.0 MiB           1       print("Loading data.")
   106    771.5 MiB     37.0 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    785.5 MiB     14.0 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    785.5 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    819.2 MiB     33.7 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    833.6 MiB     14.4 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    833.6 MiB      0.0 MiB           1       print("Training and testing.")
   117    833.6 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    833.6 MiB      0.0 MiB           1       model = getModel(args)
   120    867.3 MiB     33.7 MiB           1       model.fit(train_x, train_y)
   121    837.6 MiB    -29.7 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    839.4 MiB      1.8 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    839.4 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    839.4 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    853.9 MiB     14.4 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    853.9 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    853.9 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    854.5 MiB      0.2 MiB       89934           for i in range(len(test_id)):
   133    854.5 MiB      0.4 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    854.5 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    741.4 MiB    741.4 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    741.4 MiB      0.0 MiB           1       print("Loading data.")
   106    780.9 MiB     39.5 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    808.0 MiB     27.1 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    808.0 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    875.3 MiB     67.3 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    904.1 MiB     28.8 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    904.1 MiB      0.0 MiB           1       print("Training and testing.")
   117    904.1 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    904.1 MiB      0.0 MiB           1       model = getModel(args)
   120    954.7 MiB     50.6 MiB           1       model.fit(train_x, train_y)
   121    908.2 MiB    -46.5 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    908.2 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    908.2 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    908.2 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    929.8 MiB     21.6 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    929.8 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    929.8 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    930.0 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    930.0 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    930.0 MiB      0.0 MiB           1       return None



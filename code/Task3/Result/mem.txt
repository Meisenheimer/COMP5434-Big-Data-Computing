Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    797.7 MiB    797.7 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    797.7 MiB      0.0 MiB           1       print("Loading data.")
   106    848.7 MiB     51.0 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    865.7 MiB     17.0 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    865.7 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    865.7 MiB     -0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    865.7 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    865.7 MiB      0.0 MiB           1       print("Training and testing.")
   117    865.7 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    865.7 MiB      0.0 MiB           1       model = getModel(args)
   120    915.5 MiB     49.8 MiB           1       model.fit(train_x, train_y)
   121    896.3 MiB    -19.2 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    896.3 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    896.3 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    896.3 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    883.4 MiB    -12.9 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    883.4 MiB     -0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    883.4 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    883.5 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    883.5 MiB      0.1 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    883.5 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    796.9 MiB    796.9 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    796.9 MiB      0.0 MiB           1       print("Loading data.")
   106    833.9 MiB     37.0 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    862.6 MiB     28.6 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    862.6 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    862.6 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    862.6 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    862.6 MiB      0.0 MiB           1       print("Training and testing.")
   117    862.6 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    862.6 MiB      0.0 MiB           1       model = getModel(args)
   120    896.4 MiB     33.8 MiB           1       model.fit(train_x, train_y)
   121    883.5 MiB    -12.9 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    883.5 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    883.5 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    883.5 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    898.0 MiB     14.5 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    898.0 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    898.0 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    898.2 MiB     -0.7 MiB       89934           for i in range(len(test_id)):
   133    898.2 MiB     -0.6 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    898.2 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.0 MiB    735.0 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.0 MiB      0.0 MiB           1       print("Loading data.")
   106    785.4 MiB     50.4 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    818.0 MiB     32.6 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    818.0 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    818.0 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    818.0 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    818.0 MiB      0.0 MiB           1       print("Training and testing.")
   117    818.0 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    818.0 MiB      0.0 MiB           1       model = getModel(args)
   120    864.6 MiB     46.6 MiB           1       model.fit(train_x, train_y)
   121    845.3 MiB    -19.2 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    845.7 MiB      0.4 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    845.7 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    845.7 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    865.7 MiB     20.0 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    865.7 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    865.7 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    865.9 MiB     -1.1 MiB       89934           for i in range(len(test_id)):
   133    865.8 MiB     -1.0 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    865.9 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.4 MiB    735.4 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.4 MiB      0.0 MiB           1       print("Loading data.")
   106    772.3 MiB     36.9 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    800.1 MiB     27.7 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    800.1 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    800.1 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    800.1 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    800.1 MiB      0.0 MiB           1       print("Training and testing.")
   117    800.1 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    800.1 MiB      0.0 MiB           1       model = getModel(args)
   120    833.9 MiB     33.8 MiB           1       model.fit(train_x, train_y)
   121    821.0 MiB    -12.9 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    821.0 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    821.0 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    821.0 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    835.5 MiB     14.5 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    835.5 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    835.5 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    835.7 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    835.7 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    835.7 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    797.4 MiB    797.4 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    797.4 MiB      0.0 MiB           1       print("Loading data.")
   106    836.4 MiB     39.0 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    863.2 MiB     26.8 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    863.2 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    863.2 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    863.2 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    863.2 MiB      0.0 MiB           1       print("Training and testing.")
   117    863.2 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    863.2 MiB      0.0 MiB           1       model = getModel(args)
   120    897.2 MiB     34.0 MiB           1       model.fit(train_x, train_y)
   121    884.4 MiB    -12.9 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    884.4 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    884.4 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    884.4 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    898.9 MiB     14.5 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    898.9 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    898.9 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    898.9 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    898.9 MiB      0.1 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    898.9 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.8 MiB    735.8 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.8 MiB      0.0 MiB           1       print("Loading data.")
   106    774.0 MiB     38.2 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    786.3 MiB     12.3 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    786.3 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    786.3 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    786.3 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    786.3 MiB      0.0 MiB           1       print("Training and testing.")
   117    786.3 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    786.3 MiB      0.0 MiB           1       model = getModel(args)
   120    820.2 MiB     33.8 MiB           1       model.fit(train_x, train_y)
   121    807.3 MiB    -12.9 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    808.9 MiB      1.6 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    808.9 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    808.9 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    823.4 MiB     14.5 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    823.4 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    823.4 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    823.6 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    823.6 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    823.6 MiB      0.0 MiB           1       return None


Filename: D:\Workspace\Projects\COMP5434-Big-Data-Computing\code\Task3\main.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102    735.4 MiB    735.4 MiB           1   @profile(stream=open("./Result/mem.txt", "a", encoding="UTF-8"))
   103                                         def test(args: argparse.Namespace):
   104                                             # load the data from csv file.
   105    735.4 MiB      0.0 MiB           1       print("Loading data.")
   106    773.5 MiB     38.1 MiB           1       train_x, train_y, _ = loadData(os.path.join(args.data_dir, "train.csv"), args.feat_selection, True)
   107    801.3 MiB     27.8 MiB           1       test_x, test_id = loadData(os.path.join(args.data_dir, "test.csv"), args.feat_selection, False)
   108                                         
   109                                             # Preprocessing
   110    801.3 MiB      0.0 MiB           1       print("Preprocessing")
   111                                         
   112    801.3 MiB      0.0 MiB           1       train_x, train_y = preprocess(train_x, train_y, args, False)
   113    801.3 MiB      0.0 MiB           1       test_x = preprocess(test_x, np.zeros_like((test_x.shape[0])), args, True)
   114                                         
   115                                             # Training and testing.
   116    801.3 MiB      0.0 MiB           1       print("Training and testing.")
   117    801.3 MiB      0.0 MiB           1       score = 0.0
   118                                         
   119    801.3 MiB      0.0 MiB           1       model = getModel(args)
   120    835.1 MiB     33.8 MiB           1       model.fit(train_x, train_y)
   121    822.2 MiB    -12.9 MiB           1       pred_y = model.predict(train_x)
   122                                         
   123    822.2 MiB      0.0 MiB           1       score = f1_score(train_y, pred_y, average="macro")
   124                                         
   125    822.2 MiB      0.0 MiB           1       print("Train Score = %f." % score, file=args.log)
   126    822.2 MiB      0.0 MiB           1       print("Train Score = %f." % score)
   127                                         
   128    836.7 MiB     14.5 MiB           1       pred_y = model.predict(test_x)
   129                                         
   130    836.7 MiB      0.0 MiB           1       with open(os.path.join(args.output_dir, "res.csv"), "w") as fp:
   131    836.7 MiB      0.0 MiB           1           print("id,label", file=fp)
   132    836.9 MiB      0.0 MiB       89934           for i in range(len(test_id)):
   133    836.9 MiB      0.2 MiB       89933               print(f"{test_id[i]},{int(pred_y[i])}", file=fp)
   134                                         
   135    836.9 MiB      0.0 MiB           1       return None


